{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll build upon our earlier lesson in the basic_web_plus_api_call_interactions lesson. Remember during that lesson we learned about the webbrowser module, learned how to properly format a URL to use webbrowser, and followed that with using an API call to request some basic information from a web page.  However not all content you may want to download from a script will have an API that you can directly interact with.  Sometimes you will have to actually scrape that content for what you are looking to find.\n",
    "\n",
    "Reference:\n",
    "https://automatetheboringstuff.com/chapter11/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fairly simliar to the last lesson with requests, we need to install the module\n",
    "'''\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for below\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we'll have to import this module.  Note that up until now everything we've used we haven able to import it by the name we installed it with.  Unfortunantely, beautifulsoup4 is not this way.  Take a moment to read the below:\n",
    "\n",
    "https://docs.python.org/3/tutorial/modules.html#the-module-search-path\n",
    "\n",
    "Let's take a step and look through this.  Everything you installed with pip gets installed under the site packages directory in your PYTHON_PATH.  So you *should* be able to do the following and see beautifulsoup (somewhere in the results) assuming you are on python3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('venv/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot to look through, let's see if we can at this without hard coding our python version in and without opening a file browser to look for this manually.  Let's inspect the contents of PYTHONPATH.  In your command prompt or bash shell, that is the variable.  In your Python code it's known as sys.path so you can just get the list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew, that's still a bunch of directories to look through. Let's see if we can get through them in just a few lines of code and find beautifulsoup4.  Since we didn't see a module named \"beautifulsoup4\" above let's look through for modules that start with the letter \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in sys.path:\n",
    "    if os.path.exists(dir):\n",
    "        print([x for x in os.listdir(dir) if x.startswith('b')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a bit of a pain, but above I see the \"bs4\" directory...\n",
    "\n",
    "Let's just look at the internet for the documentation on beautifulsoup4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"https://pypi.org/search/?q=beautifulsoup4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the story, modules don't always get installed with the same name that you installed for the package name. Even if you ask pip what it knows about beautifulsoup4 you won't see that it's actually called 'bs4'. Sadly this is a Python packaging nuance and you'll find it's easiest just to reference PyPi.org for information regarding the package name or search for information about it. It's usually a bit quicker than trying to guess or look through directories on what the package name actually is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show -v beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import this along with requests from yesterday and move on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by learning a bit about HTML. HTML is the Hypertext Markup Language. Text in HTML is surrounded by tags which tell your browser how to render the content. Take a look here to learn a bit more about HTML.\n",
    "\n",
    "Reference: https://www.dataquest.io/blog/web-scraping-tutorial-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED YET - Add html/head/title/paragraph tags\n",
    "test = '''\n",
    "\n",
    "This is the title!\n",
    "\n",
    "This is in the first paragraph element\n",
    "\n",
    "This is in the second paragraph element\n",
    "\n",
    "This is in the third paragraph tag\n",
    "\n",
    "This is the fourth paragraph and it's important so put bold tags on it too\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some soup!\n",
    "soup = bs4.BeautifulSoup(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED YET - find all paragraph tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you got the basic idea on how to request all the paragraph elements from a page you wrote. So let's take this to the next step and parse out content of a page you didn't write. However you can expect a consistent output to look for results. We'll be doing a bit of scraping of stock quotes.  Take a look here for more.\n",
    "\n",
    "Reference:  http://altitudelabs.com/blog/web-scraping-with-python-and-beautiful-soup/\n",
    "\n",
    "However, I want you do a little research on Bloomberg and get your program to print out the \"Dow Jones Industrial Average\" quote rather than what is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT IMPLEMENTED YET - Define Dow Jones Industrial Average URL\n",
    "\n",
    "# NOT IMPLEMENTED YET - Get the HTML of the page using requests\n",
    "\n",
    "# NOT IMPLEMENTED YET - Parse the text and get the name and the price\n",
    "\n",
    "# NOT IMPLEMENTED YET - Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference for this exercise handed you the fields to search for. Take a moment to do the following and examine the developer tools built into the browser. You can get to this by clicking the settings -> more tools -> developer tools (on Chrome). Search for the companyName and priceText fields."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
